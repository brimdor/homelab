description = "Execute all action items from maintenance issues until all infrastructure layers are GREEN"
sync_locations = [
  ".agent/workflows/homelab-action.md",
  ".opencode/command/homelab-action.md",
  ".gemini/commands/homelab-action.toml"
]

prompt = '''

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

The text the user typed after the command **is** their priority input - it may specify:
- A specific issue number to work on
- Specific action items to prioritize
- Components to focus on first
- Any special instructions or constraints

---

# Homelab Action Workflow

## Overview
This workflow iterates on all action items in the latest maintenance-labeled Gitea issue, executing remediation steps until all infrastructure layers achieve GREEN status. It operates in a continuous loop: execute actions, validate via recon, iterate on findings until success.

> [!IMPORTANT]
> **Sync Requirement**: This workflow exists in multiple locations that must stay synchronized:
> - `.agent/workflows/homelab-action.md`
> - `.opencode/command/homelab-action.md`
> - `.gemini/commands/homelab-action.toml`
>
> When updating this file, **always copy changes to all locations**.

> [!CAUTION]
> **FOUNDATIONAL RULES APPLY** - See `_foundational-rules.md`
>
> This workflow follows the **Homelab Foundational Rules**. These rules are NON-NEGOTIABLE:
> - **Rule 1**: Work is NOT complete until ALL layers are GREEN
> - **Rule 2**: Zero tolerance for issues of ANY severity (CRITICAL through LOW)
> - **Rule 3**: NO pause, NO stop, NO quit until complete
> - **Rule 4**: Continuous validation after every action
> - **Rule 5**: All action items must be executed and verified
>
> **The ONLY exit condition is ALL GREEN status across all layers with ZERO remaining issues.**

> [!IMPORTANT]
> **Do NOT add comments to issues.** Comments are reserved for humans only.
> Always **edit the original issue body** to mark items as completed and update status.

> [!CAUTION]
> **Acceptance Criteria**: This workflow is NOT complete until:
> - All action items are resolved (checked off)
> - All infrastructure layers are **GREEN**
> - Overall status is **GREEN**
> - Final recon validation confirms GREEN status

## References
- **Documentation**: https://homelab.eaglepass.io
- **Primary Repo**: https://git.eaglepass.io/ops/homelab
- **Fallback Repo**: https://github.com/brimdor/homelab (auto-synced from primary)
- **Related Workflow**: `homelab-recon.md` (for validation)

---

## Phase 1: Prerequisites & Issue Discovery

### 1.1 Verify Access
Before starting, confirm access to all required systems:

```bash
# Verify kubectl access
kubectl cluster-info

# Verify controller access
ssh -o ConnectTimeout=5 brimdor@10.0.50.120 "echo 'Controller accessible'"

# Verify Gitea API access
source ~/.config/gitea/.env
curl -s "https://git.eaglepass.io/api/v1/user" -H "Authorization: token $GITEA_TOKEN" | jq -r '.login'
```

### 1.2 Locate Maintenance Issue
Find the latest open issue with the `maintenance` label:

```bash
source ~/.config/gitea/.env
curl -s "https://git.eaglepass.io/api/v1/repos/ops/homelab/issues?state=open&labels=maintenance" \
  -H "Authorization: token $GITEA_TOKEN" | jq -r '.[0] | "Issue #\(.number): \(.title)"'
```

### 1.3 Parse Action Items
Extract all unchecked action items (`- [ ]`) from the issue body:

1. Fetch the issue body
2. Parse markdown checkboxes
3. Categorize by phase/priority
4. Create execution plan

**Action Item Categories**:
| Priority | Description | Examples |
|----------|-------------|----------|
| **CRITICAL** | Immediate action required | Ceph crashes, nodes down, data at risk |
| **HIGH** | Should be addressed soon | Version upgrades, security patches |
| **MEDIUM** | Can be scheduled | Dependency updates, optimizations |
| **LOW** | Nice to have | Cleanup, documentation |

---

## Phase 2: Action Execution Loop

### 2.1 Execution Strategy

For each action item, follow this pattern:

```
1. READ    - Understand the action and its context
2. PLAN    - Determine the safest approach
3. BACKUP  - Ensure rollback is possible (if applicable)
4. EXECUTE - Perform the action
5. VERIFY  - Confirm the action succeeded
6. UPDATE  - Mark checkbox complete in issue
7. NEXT    - Move to next action item
```

### 2.2 Common Action Patterns

#### Ceph Storage Actions

**Archive Crash History**:
```bash
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph crash archive-all
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph health detail
```

**Check OSD Status**:
```bash
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph osd tree
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph osd df
```

**Reweight OSD**:
```bash
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph osd reweight <osd_id> 1.0
```

**Remove Dead OSD**:
```bash
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph osd out <osd_id>
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph osd purge <osd_id> --yes-i-really-mean-it
```

**Clear noout Flag**:
```bash
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph osd unset noout
```

**Check Ceph Health**:
```bash
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph status
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph health detail
```

#### Kubernetes Actions

**Check Node Status**:
```bash
kubectl get nodes -o wide
kubectl top nodes
kubectl describe node <node_name> | grep -A5 Conditions
```

**Restart Deployment**:
```bash
kubectl rollout restart deployment/<name> -n <namespace>
kubectl rollout status deployment/<name> -n <namespace>
```

**Check Pod Logs**:
```bash
kubectl logs -n <namespace> <pod_name> --tail=100
kubectl logs -n <namespace> <pod_name> --previous  # If crashed
```

**Force Pod Recreation**:
```bash
kubectl delete pod <pod_name> -n <namespace>
# Pod will be recreated by controller
```

#### Node Access Actions

**SSH to Node** (via Controller):
```bash
# Step 1: SSH to Controller
ssh brimdor@10.0.50.120

# Step 2: Start tools container
cd ~/homelab && make tools

# Step 3: Wait for container, then SSH to node
ssh root@<node_ip>
```

**Check Disk Space on Node**:
```bash
# From inside tools container
ssh root@<node_ip> "df -h"
ssh root@<node_ip> "du -sh /var/lib/rook/* 2>/dev/null | sort -hr | head -10"
```

#### ArgoCD Actions

**Sync Application**:
```bash
argocd app sync <app_name>
argocd app wait <app_name>
```

**Check Application Status**:
```bash
kubectl get applications -n argocd
argocd app get <app_name>
```

**Refresh Application**:
```bash
argocd app refresh <app_name>
```

#### Gitea PR & Issue Actions

When the maintenance issue includes Gitea action items (PRs to merge, issues to close), use these patterns:

**Merge a Pull Request**:
```bash
source ~/.config/gitea/.env

# Check PR status first
curl -s "https://git.eaglepass.io/api/v1/repos/ops/homelab/pulls/{pr_number}" \
  -H "Authorization: token $GITEA_TOKEN" | jq '{mergeable, merged, state}'

# Merge the PR (if mergeable)
curl -s -X POST "https://git.eaglepass.io/api/v1/repos/ops/homelab/pulls/{pr_number}/merge" \
  -H "Authorization: token $GITEA_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"Do": "merge", "delete_branch_after_merge": true}'
```

Or use MCP tools:
- Use `gitea_get_pull_request_by_index` to check PR status
- Merge via API if tools don't support direct merge

**Close an Issue**:
```bash
source ~/.config/gitea/.env

curl -s -X PATCH "https://git.eaglepass.io/api/v1/repos/ops/homelab/issues/{issue_number}" \
  -H "Authorization: token $GITEA_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"state": "closed"}'
```

Or use MCP tools:
- Use `gitea_edit_issue` with `state: closed`

### 2.3 Safety Guidelines

> [!WARNING]
> **Before executing destructive actions:**
> 1. Verify the target is correct
> 2. Ensure backups exist
> 3. Have a rollback plan
> 4. Consider scheduling a maintenance window

**Never do without confirmation**:
- Delete PVCs with data
- Remove OSDs without understanding why they're down
- Upgrade multiple components simultaneously
- Make changes during active incidents

**Always do**:
- One action at a time
- Verify after each action
- Update issue with progress
- Pause if unexpected behavior occurs

### 2.4 Renovate PR Processing Protocol

> [!CAUTION]
> **Renovate PRs MUST be processed one at a time.**
> Each merge requires GREEN status validation before proceeding to the next.
> If any layer goes YELLOW or RED, troubleshoot until GREEN before continuing.

#### 2.4.1 Merge Order (Safest to Riskiest)

Process Renovate PRs in this order to minimize risk:

| Order | Category | Examples | Risk Level |
|:-----:|----------|----------|:----------:|
| 1 | Non-major bundles | "update all non-major dependencies" | LOW |
| 2 | Platform services | kured, cloudflared, renovate | LOW |
| 3 | Monitoring | grafana, prometheus, loki | MEDIUM |
| 4 | Core infrastructure | argocd, external-secrets, cert-manager | HIGH |
| 5 | App templates | app-template, common libraries | MEDIUM |
| 6 | Databases | postgres, mariadb, mongodb, redis | CRITICAL |

#### 2.4.2 Per-Merge Validation

After EACH Renovate PR merge, verify ALL layers:

```bash
# Quick validation - ALL must pass before next merge
kubectl get nodes | grep -v "Ready"                           # Empty = GREEN
kubectl get pods -n kube-system | grep -v "Running\|Completed" # Empty = GREEN
kubectl get applications -n argocd | grep -v "Synced.*Healthy" # Empty = GREEN
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph health # HEALTH_OK = GREEN
kubectl get pods -A --no-headers | grep -v "Running\|Completed" # Empty = GREEN
```

**If any check fails:**
1. **STOP** - Do NOT merge next PR
2. **Investigate** - Check logs, events, describe resources
3. **Fix** - Apply remediation
4. **Validate** - Re-run all checks
5. **Only proceed** when ALL GREEN

---

## Phase 3: Validation Loop

### 3.1 Quick Health Check
After each significant action, perform a quick validation:

```bash
# Nodes healthy?
kubectl get nodes | grep -v "Ready"

# Core pods healthy?
kubectl get pods -n kube-system | grep -v "Running\|Completed"

# ArgoCD apps healthy?
kubectl get applications -n argocd | grep -v "Synced.*Healthy"

# Ceph healthy?
kubectl exec -n rook-ceph deploy/rook-ceph-tools -- ceph health
```

### 3.2 Layer Status Check

| Layer | Quick Check Command | GREEN Criteria |
|-------|---------------------|----------------|
| Metal | `kubectl get nodes` | All nodes Ready |
| System | `ceph health` + `kubectl get pods -n kube-system` | HEALTH_OK, all pods Running |
| Platform | `kubectl get applications -n argocd` | All Synced & Healthy |
| Apps | `kubectl get pods --all-namespaces \| grep -v Running` | No failed pods |

### 3.3 Full Recon Trigger
When all action items are complete, run the full recon workflow:

```
Execute: homelab-recon workflow
```

**Expected Outcome**:
- All layers: GREEN
- Overall status: GREEN
- No new action items generated

### 3.4 Iteration Decision

After recon, evaluate:

| Recon Result | Action |
|--------------|--------|
| All GREEN | Workflow complete! Close issue. |
| New YELLOW items | Add to action list, continue Phase 2 |
| New RED items | Prioritize immediately, continue Phase 2 |
| Same issues persist | Investigate root cause, escalate if needed |

---

## Phase 4: Issue Update & Completion

### 4.1 Update Issue During Execution
As each action completes, update the issue:

1. Change `- [ ]` to `- [x]` for completed items
2. Add outcome notes if relevant
3. Update timestamps
4. Keep running tally of progress

### 4.2 Final Issue Update with Exhaustive Closure Notes
When all layers are GREEN, prepare comprehensive closure documentation:

1. **Update issue title** to include `[RESOLVED]`
2. **Add exhaustive resolution summary** at top of body
3. **Document every action taken** with timestamps and outcomes
4. **Document all PRs merged/closed** during maintenance
5. **Document all Issues closed** during maintenance
6. **Include final verification data** from recon
7. **List related issues** being closed
8. **Add lessons learned** if applicable

### 4.3 Close Main Issue
```bash
source ~/.config/gitea/.env

# Update title to [RESOLVED]
curl -s -X PATCH "https://git.eaglepass.io/api/v1/repos/ops/homelab/issues/{issue_number}" \
  -H "Authorization: token $GITEA_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"title": "[RESOLVED] Original Title Here"}'

# Close the issue
curl -s -X PATCH "https://git.eaglepass.io/api/v1/repos/ops/homelab/issues/{issue_number}" \
  -H "Authorization: token $GITEA_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"state": "closed"}'
```

---

## Phase 5: Major Version Upgrade Safety Procedures

> [!CAUTION]
> **Major version upgrades** (e.g., v1 → v2, v8 → v9) often contain breaking changes.
> These require additional safety measures beyond standard action execution.

### 5.1 Pre-Upgrade Assessment

Before ANY major version upgrade:

**Questions to answer:**
- [ ] Are there API/CRD schema changes?
- [ ] Are there deprecated features being removed?
- [ ] Are there configuration file format changes?
- [ ] Are there data migration requirements?
- [ ] Is there a recommended upgrade path (e.g., v7 → v8 → v9)?

### 5.2 Staged Upgrade Process

For HIGH/CRITICAL risk upgrades, use this staged approach:

```
[Stage 1: Prepare]     → Document, backup, validate rollback
[Stage 2: Dry Run]     → helm template, terraform plan, review diffs
[Stage 3: Canary]      → Upgrade in test/dev if available
[Stage 4: Execute]     → Upgrade with monitoring
[Stage 5: Validate]    → Full health check, functionality test
[Stage 6: Stabilize]   → Monitor for 30+ minutes before next upgrade
```

---

## Phase 6: Database Upgrade Protocols

> [!DANGER]
> **Database upgrades carry the highest risk of data loss.**
> NEVER proceed without verified, restorable backups.

### 6.1 Database Upgrade Pre-Flight Checklist

Before upgrading ANY database (PostgreSQL, MariaDB, MongoDB, Redis, Valkey):

1. Verify current state and version
2. Create full backup
3. Verify backup integrity
4. Document recovery procedure

### 6.2 Database Upgrade Execution

**Sequence for database upgrades:**

```
1. [STOP]      Scale down all applications using the database
2. [BACKUP]   Create and verify full backup
3. [SNAPSHOT] Create PVC snapshot if available
4. [UPGRADE]  Apply the version upgrade
5. [WAIT]     Wait for pod to be Running (may take 5-10 min for migrations)
6. [VERIFY]   Connect and verify data integrity
7. [START]    Scale up applications one at a time
8. [VALIDATE] Verify applications function correctly
```

---

## Phase 7: Escalation Procedures

### 7.1 When to Escalate
Escalate to human intervention when:

- Action requires physical hardware access
- Data loss risk is present
- Credentials or secrets need rotation
- Network infrastructure changes needed
- Multiple failures cascade
- Root cause cannot be determined
- Action has been attempted 3+ times without success

### 7.2 Escalation Actions

1. **Document the issue** clearly in the Gitea issue
2. **Add `needs-human` label** if available
3. **Stop automated actions** that might make things worse
4. **Preserve logs and state** for debugging
5. **Summarize findings** for human review

### 7.3 Safe States
If unable to proceed, ensure the cluster is in a safe state:

- Ceph `noout` flag set (prevents data movement)
- No pending destructive operations
- Critical services are running
- Monitoring is active

---

## Execution Checklist

- [ ] Phase 1: Access verified, maintenance issue located
- [ ] Phase 2: Action items parsed and prioritized
- [ ] Phase 2: Actions executed one-by-one with verification
- [ ] Phase 2: Gitea PRs merged/closed as specified in maintenance issue
- [ ] Phase 3: Quick health checks passed after each action
- [ ] Phase 3: Full recon validation performed
- [ ] Phase 3: All layers confirmed GREEN
- [ ] Phase 4: Issue updated with progress throughout
- [ ] Phase 4: Exhaustive closure notes added to main issue
- [ ] Phase 4: Main maintenance issue closed
- [ ] Overall Status: **GREEN**

---

## Success Criteria

The workflow is complete when:

1. **All action items** in the maintenance issue are checked off
2. **Metal Layer**: GREEN - All nodes Ready, resources healthy
3. **System Layer**: GREEN - Ceph HEALTH_OK, core pods healthy
4. **Platform Layer**: GREEN - All ArgoCD apps Synced/Healthy
5. **Apps Layer**: GREEN - All application pods running
6. **Overall Status**: GREEN
7. **Final recon report**: Confirms GREEN status
8. **All PRs actioned**: Merged or closed as specified in maintenance issue
9. **Exhaustive closure notes**: Added to main issue with full action log
10. **Main maintenance issue**: Closed with `[RESOLVED]` title

---

## Notes

- Always prioritize data safety over speed
- One action at a time, verify before proceeding
- Update the issue frequently for visibility
- If in doubt, pause and assess
- This workflow complements `homelab-recon.md` - use recon to validate
- Human escalation is a valid outcome for complex issues
'''
