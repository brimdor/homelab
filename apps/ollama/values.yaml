# Default values for ollama-helm.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

ollama:
  # -- Number of replicas
  replicaCount: 1

  # Docker image
  image:
    # -- Docker image registry
    repository: ollama/ollama

    # -- Docker pull policy
    pullPolicy: Always

    # -- Docker image tag, overrides the image tag whose default is the chart appVersion.
    tag: "latest"

  # -- Docker registry secret names as an array
  imagePullSecrets: []

  # -- String to partially override template  (will maintain the release name)
  nameOverride: ""

  # -- String to fully override template
  fullnameOverride: ""

  # -- String to fully override namespace
  namespaceOverride: ""

  # Ollama parameters
  ollama:
    # Port Ollama is listening on
    port: 11434

    gpu:
      # -- Enable GPU integration
      enabled: false

      # -- Enable DRA GPU integration
      # If enabled, it will use DRA instead of Device Driver Plugin and create a ResourceClaim and GpuClaimParameters
      draEnabled: false

      # -- DRA GPU DriverClass
      draDriverClass: "gpu.nvidia.com"

      # -- Existing DRA GPU ResourceClaim Template
      draExistingClaimTemplate: ""

      # -- GPU type: 'nvidia' or 'amd'
      # If 'ollama.gpu.enabled', default value is nvidia
      # If set to 'amd', this will add 'rocm' suffix to image tag if 'image.tag' is not override
      # This is due cause AMD and CPU/CUDA are different images
      type: 'nvidia'

      # -- Specify the number of GPU
      # If you use MIG section below then this parameter is ignored
      number: 1

      # -- only for nvidia cards; change to (example) 'nvidia.com/mig-1g.10gb' to use MIG slice
      nvidiaResource: "nvidia.com/gpu"
      # nvidiaResource: "nvidia.com/mig-1g.10gb" # example
      # If you want to use more than one NVIDIA MIG you can use the following syntax (then nvidiaResource is ignored and only the configuration in the following MIG section is used)

      mig:
        # -- Enable multiple mig devices
        # If enabled you will have to specify the mig devices
        # If enabled is set to false this section is ignored
        enabled: false

        # -- Specify the mig devices and the corresponding number
        devices: {}
            #        1g.10gb: 1
            #        3g.40gb: 1

    models:
      # -- List of models to pull at container startup
      # The more you add, the longer the container will take to start if models are not present
      # pull:
      #  - llama2
      #  - mistral
      # Limit pulled models to a small 8b variant to avoid GPU OOM
      pull:
        # - qwen3:8b
      # -- List of models to load in memory at container startup
      # run:
      #  - llama2
      #  - mistral
      # Load only the Echo model (created from a small 8b base) to keep memory low
      run:
        # - Echo

      # -- List of models to create at container startup, there are two options
      # 1. Create a raw model
      # 2. Load a model from configMaps, configMaps must be created before and are loaded as volume in "/models" directory.
      # create:
      #  - name: llama3.1-ctx32768
      #    configMapRef: my-configmap
      #    configMapKeyRef: configmap-key
      #  - name: llama3.1-ctx32768
      #    template: |
      #      FROM llama3.1
      #      PARAMETER num_ctx 32768
      # NOTE: Avoid using the chart's built-in `create` lifecycle hook.
      # It runs as a `postStart` hook inside the main ollama pod, and any
      # transient failure will crashloop the deployment.
      create: []

      # -- Automatically remove models present on the disk but not specified in the values file
      clean: false

    # -- Add insecure flag for pulling at container startup
    insecure: false

    # -- Override ollama-data volume mount path, default: "/root/.ollama"
    mountPath: ""

  # Service account
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  serviceAccount:
    # -- Specifies whether a service account should be created
    create: true

    # -- Automatically mount a ServiceAccount's API credentials?
    automount: true

    # -- Annotations to add to the service account
    annotations: {}

    # -- The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""

  # -- Map of annotations to add to the pods
  podAnnotations: {}

  # -- Map of labels to add to the pods
  podLabels: {}

  # -- Pod Security Context
  podSecurityContext: {}
    # fsGroup: 2000

  # -- Priority Class Name
  priorityClassName: ""

  # -- Container Security Context
  securityContext: {}
    # capabilities:
    #  drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  # -- Specify runtime class
  runtimeClassName: "nvidia"

  # Configure Service
  service:

    # -- Service type
    type: ClusterIP

    # -- Service port
    port: 11434

    # -- Service node port when service type is 'NodePort'
    nodePort: 31434

    # -- Load Balancer IP address
    loadBalancerIP:

    # -- Annotations to add to the service
    annotations: {}

    # -- Labels to add to the service
    labels: {}

  # Configure Deployment
  deployment:

    # -- Labels to add to the deployment
    labels: {}

  # Configure the ingress resource that allows you to access the
  ingress:
  #   # -- Enable ingress resource
    enabled: true

  #   # -- Ingress class name
    className: nginx

  #   # -- Ingress annotations
    annotations:
      external-dns.alpha.kubernetes.io/target: "homelab-tunnel.eaglepass.io"
      external-dns.alpha.kubernetes.io/cloudflare-proxied: "true"
      cert-manager.io/cluster-issuer: "letsencrypt-prod"

  #   # -- Hosts to create ingress for
    hosts:
      - host: &host ollama.eaglepass.io
        paths:
          - path: /
            pathType: Prefix
    
  #   # -- TLS configuration
    tls:
      - secretName: ollama-tls
        hosts:
          - *host


  # Configure resource requests and limits
  # ref: http://kubernetes.io/docs/user-guide/compute-resources/
  resources:
    # -- CPU and memory resource requests and limits
    limits:
      cpu: 6
      memory: 40Gi
    requests:
      cpu: 2
      memory: 32Gi

  # Configure extra options for liveness probe
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  livenessProbe:
    # -- Enable livenessProbe
    enabled: true

    # -- Request path for livenessProbe
    path: /

    # -- Initial delay seconds for livenessProbe
    initialDelaySeconds: 60

    # -- Period seconds for livenessProbe
    periodSeconds: 10

    # -- Timeout seconds for livenessProbe
    timeoutSeconds: 5

    # -- Failure threshold for livenessProbe
    failureThreshold: 6

    # -- Success threshold for livenessProbe
    successThreshold: 1

  # Configure extra options for readiness probe
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  readinessProbe:
    # -- Enable readinessProbe
    enabled: true

    # -- Request path for readinessProbe
    path: /

    # -- Initial delay seconds for readinessProbe
    initialDelaySeconds: 30

    # -- Period seconds for readinessProbe
    periodSeconds: 5

    # -- Timeout seconds for readinessProbe
    timeoutSeconds: 3

    # -- Failure threshold for readinessProbe
    failureThreshold: 6

    # -- Success threshold for readinessProbe
    successThreshold: 1

  # Configure autoscaling
  autoscaling:
    # -- Enable autoscaling
    enabled: false

    # -- Number of minimum replicas
    minReplicas: 1

    # -- Number of maximum replicas
    maxReplicas: 100

    # -- CPU usage to target replica
    targetCPUUtilizationPercentage: 80

    # -- targetMemoryUtilizationPercentage: 80

  # -- Additional volumes on the output Deployment definition.
  volumes: []
  # -- - name: foo
  #   secret:
  #     secretName: mysecret
  #     optional: false

  # -- Additional volumeMounts on the output Deployment definition.
  volumeMounts: []
  # -- - name: foo
  #   mountPath: "/etc/foo"
  #   readOnly: true

  # -- Additional arguments on the output Deployment definition.
  extraArgs: []

  # -- Additional environments variables on the output Deployment definition.
  # For extra OLLAMA env, please refer to https://github.com/ollama/ollama/blob/main/envconfig/config.go
  extraEnv:
    - name: NVIDIA_VISIBLE_DEVICES
      value: "all"
    - name: NVIDIA_DRIVER_CAPABILITIES
      value: "all"
    - name: OLLAMA_CONTEXT_LENGTH
      value: "131072"
    - name: OLLAMA_FLASH_ATTENTION
      value: "1"
    - name: OLLAMA_KV_CACHE_TYPE
      value: "q4_0"
    # - name: OLLAMA_ORIGINS
    #   value: "*"

  # -- Additionl environment variables from external sources (like ConfigMap)
  extraEnvFrom: []
  #  - configMapRef:
  #      name: my-env-configmap

  # Enable persistence using Persistent Volume Claims
  # ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  persistentVolume:
    # -- Enable persistence using PVC
    enabled: true

    # -- Ollama server data Persistent Volume access modes
    # Must match those of existing PV or dynamic provisioner
    # Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    accessModes:
      - ReadWriteOnce

    # -- Ollama server data Persistent Volume annotations
    annotations: {}

    # -- If you'd like to bring your own PVC for persisting Ollama state, pass the name of the
    # created + ready PVC here. If set, this Chart will not create the default PVC.
    # Requires server.persistentVolume.enabled: true
    existingClaim: ""

    # -- Ollama server data Persistent Volume size
    size: 200Gi

    # -- Ollama server data Persistent Volume Storage Class
    # If defined, storageClassName: <storageClass>
    # If set to "-", storageClassName: "", which disables dynamic provisioning
    # If undefined (the default) or set to null, no storageClassName spec is
    # set, choosing the default provisioner.  (gp2 on AWS, standard on
    # GKE, AWS & OpenStack)
    storageClass: ""

    # -- Ollama server data Persistent Volume Binding Mode
    # If defined, volumeMode: <volumeMode>
    # If empty (the default) or set to null, no volumeBindingMode spec is
    # set, choosing the default mode.
    volumeMode: ""

    # -- Subdirectory of Ollama server data Persistent Volume to mount
    # Useful if the volume's root directory is not empty
    subPath: ""

    # -- Pre-existing PV to attach this claim to
    # Useful if a CSI auto-provisions a PV for you and you want to always
    # reference the PV moving forward
    volumeName: ""

  # -- Node labels for pod assignment.
  nodeSelector: {}

  # -- Tolerations for pod assignment
  tolerations:
    - key: "dedicated"
      value: "arcanine"
      effect: "NoSchedule"

  # -- Affinity for pod assignment
  # affinity:
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       nodeSelectorTerms:
  #         - matchFields:
  #             - key: metadata.name
  #               operator: In
  #               values:
  #                 - arcanine
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
                - "arcanine"

  # -- Lifecycle for pod assignment (override ollama.models startup pull/run)
  lifecycle: {}

  # How to replace existing pods
  updateStrategy:
    # -- Deployment strategy can be "Recreate" or "RollingUpdate". Default is Recreate
    type: "Recreate"
    # rollingUpdate:
    #   maxSurge: 1
    #   maxUnavailable: 0

  # -- Topology Spread Constraints for pod assignment
  topologySpreadConstraints: {}

  # -- Wait for a grace period
  terminationGracePeriodSeconds: 120

  # -- Init containers to add to the pod
  initContainers: []
  # - name: startup-tool
  #   image: alpine:3
  #   command: [sh, -c]
  #   args:
  #     - echo init

  # -- Use the host’s ipc namespace.
  hostIPC: false

  # -- Use the host’s pid namespace
  hostPID: false

  # -- Use the host's network namespace.
  hostNetwork: false

  # -- Extra K8s manifests to deploy
  # -- Extra K8s manifests to deploy
  extraObjects: []

modelPuller:
  script: |
    #!/bin/sh
    set -eu

    echo "Waiting for Ollama service at $OLLAMA_HOST..."
    start="$(date +%s)"
    timeout_secs=1200

    while ! ollama list > /dev/null 2>&1; do
      now="$(date +%s)"
      if [ $((now - start)) -gt "$timeout_secs" ]; then
        echo "Timed out waiting for ollama after ${timeout_secs}s" >&2
        exit 1
      fi
      echo "Waiting for ollama..."
      sleep 10
    done

    echo "Service ready. Starting pulls..."

    pull_with_retry() {
      model="$1"
      attempts=5
      delay=15

      i=1
      while true; do
        echo "Pulling $model (attempt ${i}/${attempts})..."
        if ollama pull "$model"; then
          return 0
        fi

        if [ "$i" -ge "$attempts" ]; then
          echo "Failed to pull $model after ${attempts} attempts" >&2
          return 1
        fi

        i=$((i + 1))
        sleep "$delay"
      done
    }

    # Only request the smaller 8b variant to avoid large downloads / GPU memory usage
    models="qwen3:8b nomic-embed-text:latest mxbai-embed-large:latest gpt-oss:20b"
    for model in $models; do
      pull_with_retry "$model"
    done

    echo "Creating Echo model (smaller base, 128k context, fixed reasoning)..."
    cat <<'EOF' > /tmp/Modelfile.Echo
    FROM qwen3:8b
    PARAMETER num_ctx 131072
    PARAMETER repeat_penalty 1.5
    SYSTEM """
    ROLE: Principal Systems Reliability Engineer (SRE)
    OBJECTIVE: Autonomously manage and maintain a hybrid Kubernetes Homelab environment with production-grade precision.
      **Identity & Persona**
      You are **Echo**, the Principal Systems Reliability Engineer (SRE) responsible for the autonomous management and health of the homelab. You are the primary engine of execution, providing the exact code, commands, and configurations required to maintain a state-of-the-art production environment.

      **The Prime Directive: Lab Health**
      Maintain the absolute operational integrity of the cluster. Ensure every critical action is preceded by a verified backup or a documented recovery path to safeguard data and service availability.

      **Engineering Persistence**
      Maintain a relentless focus on resolution. When encountering roadblocks—such as failing deployments, flaky ingresses, or degraded nodes—pursue the fix through every layer of the stack (manifests, networking, and logs) until the service reaches a confirmed "Healthy" state.

      **Technical Ecosystem**
      - **Infrastructure:** K3s, Talos OS, Ansible, Terraform/OpenTofu.
      - **Continuous Delivery:** FluxCD (GitOps).
      - **Observability:** Prometheus, Grafana, VictoriaMetrics.
      - **Networking:** Cloudflare Tunnels, MetalLB, Traefik, Cilium, Pi-hole.
      - **Security:** SOPS, Vault, External Secrets Operator.

      **Execution Protocols**
      1. **The GitOps Standard:** Perform all permanent cluster changes by generating manifest updates for the Git repository to ensure it remains the single source of truth.
      2. **Persistence Loop:** Provide a complete sequence for multi-step tasks. In the event of an error, analyze the output immediately and provide the necessary pivot-fix to continue the mission.
      3. **Pre-Flight Validation:** Include a validation or dry-run command (e.g., 'flux reconcile --dry-run', 'terraform plan') for every state-changing action.
      4. **Idempotent Delivery:** Ensure all code, scripts, and manifests are safe for repeated execution.
      5. **Schema Compliance:** When calling tools (e.g., bash, shell, or kubernetes tools), always include a mandatory 'description' field as a string. This description must clearly state the intent of the action to satisfy validation requirements.
      6. **Direct Output Requirement:**
        - Wrap every individual terminal command in its own codeblock.
        - Wrap all data outputs (YAML, JSON, Markdown, etc.) in codeblocks.
        - Specify the exact file path and filename above configuration blocks.

      **Operational Mindset**
      You are the engineer at the keyboard. Be resilient, methodical, and proactive. Stay focused on the "Healthy" state of the lab and drive every reconciliation to a successful conclusion.
    """
    EOF

    echo "Removing existing Echo model to enforce update..."
    ollama rm Echo || true

    if ollama create Echo -f /tmp/Modelfile.Echo; then
        echo "Successfully created Echo model"
    else
        echo "Failed to create Echo model" >&2
    fi

    echo "All models requested. Sleeping indefinitely..."
    sleep infinity

  # Test connection pods
  tests:
    enabled: true
    # -- Labels to add to the tests
    labels: {}
    # -- Annotations to add to the tests
    annotations: {}
