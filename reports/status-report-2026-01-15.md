# Homelab Status Report - 2026-01-15

**Generated**: 2026-01-15 13:55 CST (Updated)  
**Overall Status**: 游릭 **GREEN**

---

## Executive Summary

All cluster layers are operating nominally. No critical issues detected.

| Layer | Status | Summary |
|-------|:------:|---------|
| **Metal** | 游릭 | 10/10 nodes Ready |
| **System** | 游릭 | kube-system healthy, Ceph HEALTH_OK |
| **Platform** | 游릭 | 43/43 ArgoCD apps Synced+Healthy |
| **Apps** | 游릭 | All pods Running or Completed |

---

## Layer Details

### Metal Layer (Nodes)

| Node | Status | CPU | Memory | Role |
|------|:------:|----:|-------:|------|
| arcanine | Ready | 45% | 19% | worker |
| bulbasaur | Ready | 19% | 39% | control-plane |
| charmander | Ready | 39% | 32% | control-plane |
| chikorita | Ready | 11% | 14% | worker |
| cyndaquil | Ready | 10% | 27% | worker |
| growlithe | Ready | 11% | 29% | worker |
| pikachu | Ready | 13% | 25% | worker |
| sprigatito | Ready | 6% | 28% | worker |
| squirtle | Ready | 16% | 25% | control-plane |
| totodile | Ready | 13% | 39% | worker |

**Cluster Version**: K3s v1.33.6+k3s1  
**Kernel**: 6.11.9-100.fc39.x86_64  
**OS**: Fedora Linux 39

### System Layer (Core Services)

#### Ceph Storage
- **Status**: HEALTH_OK
- **Muted Alerts**: `DB_DEVICE_STALLED_READ_ALERT` (auto-expiring)
- **OSDs**: 7 up, 7 in
- **Storage**: 1.2 TiB / 3.4 TiB used (~35%)
- **Monitors**: 3 daemons (quorum f,h,j)
- **MDS**: 1/1 daemons up, 1 hot standby
- **PGs**: 177 active+clean

#### kube-system
- All pods Running or Completed
- Cilium CNI healthy (10 pods + operator)
- CoreDNS, Hubble, kube-vip, Metrics Server operational

### Platform Layer (GitOps)

All 43 ArgoCD applications are **Synced** and **Healthy**:
argocd, backlog, backlog-canary, budget, budget-canary, cert-manager, cloudflared, connect, dex, doplarr, emby, explorers-hub, external-dns, external-secrets, family-games-host, family-games-rules, gitea, global-secrets, gpu-operator, grafana, humbleai, humbleai-canary, ingress-nginx, kanidm, kured, localai, loki, monitoring-system, n8n, nextcloud, ollama, openwebui, postgres, qdrant, radarr, renovate, rook-ceph, sabnzbd, searxng, sonarr, volsync-system, wolf, woodpecker, zot

### Apps Layer

- All pods in Running or Completed state
- All certificates valid and Ready
- 7 ExternalSecrets synced

---

## Warning Events (Non-Critical)

| Node | Event | Description |
|------|-------|-------------|
| sprigatito | InvalidDiskCapacity | invalid capacity 0 on image filesystem |
| pikachu | InvalidDiskCapacity | invalid capacity 0 on image filesystem |
| chikorita | InvalidDiskCapacity | invalid capacity 0 on image filesystem |
| growlithe | InvalidDiskCapacity | invalid capacity 0 on image filesystem |

> [!NOTE]
> These are intermittent kubelet reporting events and do not indicate actual disk issues.

---

## Repository Status

### Open Pull Requests
**None** - All PRs merged

### Open Issues

| # | Title | Type |
|---|-------|------|
| 4 | Dependency Dashboard | Renovate auto-managed |

---

## Action Items

**No action items** - All layers GREEN, no open PRs.

### Informational Only (P3)

1. **InvalidDiskCapacity warnings** - Benign kubelet metrics, no action needed
2. **Ceph muted alert** - DB_DEVICE_STALLED on OSD.0 under observation, auto-expiring mute

---

## Verification Commands

```bash
# Quick health check
kubectl get nodes | grep -v "Ready"
kubectl get pods -A --no-headers | grep -v "Running\|Completed"
kubectl get applications -n argocd | grep -v "Synced.*Healthy"
kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph health
```

---

*Report generated by homelab-recon workflow*
