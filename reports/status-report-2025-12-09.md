# Homelab Status Report - 2025-12-09

## Executive Summary
**Overall Status**: **YELLOW**
**Summary**: Cluster is operationally healthy with all Kubernetes nodes Ready and all ArgoCD applications Synced/Healthy. However, Ceph storage is in HEALTH_WARN state with **629 daemon crashes** (primarily OSDs 0, 4, 6, 8), **mon g low on disk space (16% avail)**, and 2 OSDs currently down. The noout flag is set, likely a protective measure during troubleshooting. Immediate attention required on Ceph storage subsystem.

## Detailed Status
| Layer | Status | Key Findings |
|-------|:------:|--------------|
| **Metal** | GREEN | - Controller (10.0.50.120) reachable<br>- All 10 nodes reachable via Kubernetes API<br>- CPU usage healthy (3-20%)<br>- Memory usage healthy (10-37%) |
| **System** | YELLOW | - All 10 nodes Ready (k3s v1.28.3+k3s2)<br>- Cilium CNI healthy on all nodes<br>- Ceph HEALTH_WARN: mon g disk low (16%), noout flag set, **629 daemon crashes**<br>- 7/9 OSDs up, 5 in; OSD 1, 5 down |
| **Platform** | GREEN | - All 32 ArgoCD apps Synced & Healthy<br>- Ingress-nginx running<br>- All 17 certificates valid (Ready=True)<br>- Prometheus, Grafana, Alertmanager operational<br>- Dex & Kanidm identity providers running |
| **Apps** | GREEN | - Emby, Gitea, Sonarr, Radarr, SabNZBD running<br>- OpenWebUI, Ollama, LocalAI, HumbleAI running<br>- SearXNG, Woodpecker CI operational<br>- No pods in failed state |

## Issues & Recommendations

### Critical (Red)
None

### Warnings (Yellow)
- [ ] **Issue**: Ceph OSD daemon crash storm - 629 crashes in past 24h
  - **Affected OSDs**: 0 (chikorita), 4 (bulbasaur), 6 (growlithe), 8 (arcanine)
  - **Fix**: 
    1. Check OSD logs: `kubectl logs -n rook-ceph rook-ceph-osd-X-*`
    2. Investigate disk health on affected nodes
    3. Consider replacing failing disks or redeploying OSDs
    4. Archive crash info: `ceph crash archive-all`

- [ ] **Issue**: Monitor `g` (growlithe) low on disk space - 16% available
  - **Fix**: 
    1. Check disk usage on growlithe node
    2. Consider adding storage or cleaning up stale data
    3. Move mon to node with more capacity if needed

- [ ] **Issue**: 2 OSDs down (osd.1, osd.5), 2 more showing 0 weight (osd.6, osd.7)
  - **Fix**: 
    1. Verify disk presence on affected nodes
    2. Check if disks were removed/failed
    3. Redeploy or remove stale OSD entries

- [ ] **Issue**: `noout` flag is set on cluster
  - **Fix**: Once OSD issues resolved, clear flag: `ceph osd unset noout`

### Observations (Green)
- Kubernetes cluster fully operational with 10 nodes across 290 days
- All platform services (GitOps, Ingress, TLS, Monitoring, IdP) functioning normally
- All user applications responsive
- Storage capacity adequate: 209 GiB used / 2.5 TiB total (8%)
- Resource utilization efficient - highest CPU consumer is Ceph mgr at 85m cores

## Verification Data
- **Nodes**: 10 Online / 10 Total
- **Storage**: 8% Used (2.3 TiB free)
- **Ceph OSDs**: 7 up / 9 total, 5 in cluster
- **Top Resource Consumers**: 
  - CPU: rook-ceph-mgr-b (85m), Prometheus (77m)
  - Memory: Emby (3.9Gi), Prometheus (1.9Gi)

---
*Generated by OpenCode - Last updated: 2025-12-09 13:00*
